{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at some Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning all about tokenization and regexes, let's start doing some cool stuff and apply it in a true dataset!\n",
    "\n",
    "In Part II of this BLU, we're going to look into how to transform text into something that is meaningful to a machine. As you may have noticed, text is a bit different from other datasets you might have seen - it's just a bunch of words stringed together! Where are the features in a tightly organized table of examples? Unlike other data you might have worked with in previous BLU's, text is unstructured and thus needs some additional work on our end to make it structured and ready to be handled by a machine learning algorithm.\n",
    "\n",
    "One thing is clear - we need features. To get features from a string of text, or a **document**, is to **vectorize** it. Normally, this means that our feature space is the **vocabulary** of the examples present in our dataset, that is, the set of unique words we can find in all of the training examples.\n",
    "\n",
    "But enough talk - let's get our hands dirty!\n",
    "\n",
    "In this BLU, we're going to work with some movie reviews from IMDB. Let's load the dataset into pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Aldolpho (Steve Buscemi), an aspiring film mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>An unfunny, unworthy picture which is an undes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>A failure. The movie was just not good. It has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I saw this movie Sunday afternoon. I absolutel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Disney goes to the well one too many times as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0  Negative  Aldolpho (Steve Buscemi), an aspiring film mak...\n",
       "1  Negative  An unfunny, unworthy picture which is an undes...\n",
       "2  Negative  A failure. The movie was just not good. It has...\n",
       "3  Positive  I saw this movie Sunday afternoon. I absolutel...\n",
       "4  Negative  Disney goes to the well one too many times as ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/imdb_sentiment_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are two columns in this dataset - one for the labels and another for the text of the movie review. Each example is labeled as a positive or negative review. Our goal is to retrieve meaningful features from the text so a machine can predict if a given unlabeled review is positive or negative.\n",
    "\n",
    "Let's see a positive and a negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "If The Lion King is a serious story about a young lion growing up to avenge his father's death, The Lion King 1 and a half is the total opposite, full of whimsy and cheer. The Lion King told the story from the side of Simba the young lion, 1 and a half is from the view of Timone and Pumbaa, a less than perfect duo made up of a meercat who left home because he could not dig tunnels without burying his friends and neighbors and a warthog who has an odor issue. The movie is a little short on substance, but Disney does a good job of filling time with various sketches starring Timone and Pumbaa as they \"watch\" the movie with us. My favorite is the sing-along that happens halfway through the movie, make sure you watch the bouncing bug! Disney has advertised 1 and a half as \"the rest of the story,\" though it really isn't. It is just a different perspective of The Lion King, without all of the serious stuff that pervaded most of the second half of the original Disney classic. Credit Nathan Lane as Timone and Ernie Sabella as Pumbaa for their voice work, without their efforts, the movie may not have worked. The sing, they entertain, and they make us laugh. They also give us a reason to avoid a hot tub with a warthog.\n"
     ]
    }
   ],
   "source": [
    "pos_example = df.text[21923]\n",
    "print(df.sentiment[21923])\n",
    "print(pos_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! So here's a review about *The Lion King 1 and a 1/2* (a.k.a. *The Lion King 3* in some countries). It seems the reviewer liked it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Disney goes to the well one too many times as anybody who has seen the original LITTLE MERMAID will feel blatantly ripped off. Celebrating the birth of their daughter Melody, Ariel and Eric plan on introducing her to King Triton. The celebration is quickly crashed by Ursula 's sister, Morgana who plans to use Melody as a defense tool to get the King 's trident. Stopping the attack, Ariel and Eric build a wall around the ocean while Melody grows up wondering why she cannot go in there.<br /><br />Awful and terrible is what describes this direct to video sequel. LITTLE MERMAID 2 gives you that feeling everything you watch seemed to have come straight other Disney movies. I guess Disney can only plagiarize itself! Do not tell me that the penguin and walrus does not remind you of another duo from the LION KING!<br /><br />Other disappointing moments include the rematch between Sebastien and Louie, the royal chef. They terribly under played it! The climax between Morgana and EVERYONE seemed to be another disappointment.<br /><br />I will not give anything away, but in 75 minutes, everything seemed incredibly cramped and too much to handle. An embarrassment to Disney, LITTLE MERMAID 2 is better left to rent and laugh at. Then you can prepare for the rest of the other sequels Disney is going to drown you in later on.\n"
     ]
    }
   ],
   "source": [
    "neg_example = df.text[4]\n",
    "print(df.sentiment[4])\n",
    "print(neg_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. I guess that's a pass for this movie, right?\n",
    "\n",
    "Let's get the first 200 documents of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentiment of each of these documents is as follows:\n",
    "# POSITIVE, NEGATIVE, POSITIVE, POSITIVE, NEGATIVE, NEGATIVE\n",
    "\n",
    "# docs = [pos_example, neg_example, df.text[24949], df.text[24995], df.text[3], df.text[0]]\n",
    "docs = df.text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USE CONCEPTS OF PART I TO CLEAN THESE EXAMPLES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, our feature space in text will be the vocabulary of our data. In our example, this is the set of unique words and symbols present in our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1940.',\n",
       " 'world,',\n",
       " 'hubris',\n",
       " 'producer',\n",
       " \"mother's\",\n",
       " 'refreshing,',\n",
       " 'me....NOTHING.<br',\n",
       " 'Castleville.',\n",
       " 'describes',\n",
       " 'low-budget,',\n",
       " 'smitten.',\n",
       " 'training',\n",
       " 'paid',\n",
       " 'website)',\n",
       " 'standup,',\n",
       " 'screenplay,',\n",
       " '\"shot',\n",
       " 'whole..Riff',\n",
       " 'fictions',\n",
       " 'know:',\n",
       " '\"redemption',\n",
       " 'fear.',\n",
       " 'hint',\n",
       " 'McLean,',\n",
       " 'hands.',\n",
       " 'malaise.',\n",
       " 'Nikolai',\n",
       " 'logical',\n",
       " '15',\n",
       " 'First',\n",
       " 'brilliant.<br',\n",
       " 'pity',\n",
       " 'U.S.',\n",
       " 'nickname',\n",
       " 'club.',\n",
       " 'Perros',\n",
       " 'cramped',\n",
       " 'passing',\n",
       " 'whose',\n",
       " 'imprisons',\n",
       " 'there,',\n",
       " 'begins.<br',\n",
       " 'right,',\n",
       " 'Ethan',\n",
       " 'certain!',\n",
       " 'darthvader',\n",
       " '(which',\n",
       " '3.5/10',\n",
       " 'values',\n",
       " 'loneliness',\n",
       " 'mainstream',\n",
       " 'resembles',\n",
       " 'Clipped',\n",
       " 'confess',\n",
       " 'restroom!',\n",
       " 'one!',\n",
       " 'promotion',\n",
       " 'beneath',\n",
       " 'lesbian.',\n",
       " 'government',\n",
       " 'seedy',\n",
       " 'later',\n",
       " 'Zone\".',\n",
       " 'School',\n",
       " 'experiencing',\n",
       " 'gore',\n",
       " \"Captains'\",\n",
       " 'boot!',\n",
       " 'suited',\n",
       " 'Bobby.',\n",
       " 'rent',\n",
       " 'sees',\n",
       " 'exist',\n",
       " 'paradoxes',\n",
       " 'Harry',\n",
       " 'you;',\n",
       " 'shopworn.',\n",
       " 'sad',\n",
       " 'knocked',\n",
       " 'massive',\n",
       " 'NOT',\n",
       " 'Crystal.<br',\n",
       " 'workers.',\n",
       " 'basically,',\n",
       " 'society.',\n",
       " 'screen.',\n",
       " 'recommended,',\n",
       " 'proves',\n",
       " 'mystic',\n",
       " 'Bolt',\n",
       " 'familiar',\n",
       " \"Keith's\",\n",
       " 'Though,',\n",
       " 'Reloaded.',\n",
       " '/>long',\n",
       " 'punishment',\n",
       " 'consistent',\n",
       " 'sadly',\n",
       " 'sure,',\n",
       " 'Bin',\n",
       " 'Mulrooney?',\n",
       " 'Scout',\n",
       " 'infamous',\n",
       " 'Soll',\n",
       " 'BLOWUP:',\n",
       " 'The',\n",
       " 'folklore,',\n",
       " 'visit',\n",
       " 'Just',\n",
       " 'insects',\n",
       " 'predator',\n",
       " 'Second,',\n",
       " 'SUPREMELY',\n",
       " '\"rosy',\n",
       " 'show.',\n",
       " \"'kid'\",\n",
       " 'paces.',\n",
       " 'theatrics,',\n",
       " 'Play',\n",
       " 'could,',\n",
       " 'shine',\n",
       " 'Little',\n",
       " 'five',\n",
       " 'ingredients',\n",
       " 'broadcast',\n",
       " 'humanized',\n",
       " 'trap',\n",
       " 'meet',\n",
       " 'pick',\n",
       " 'Asano',\n",
       " 'criminals',\n",
       " 'aroused',\n",
       " 'Massey,',\n",
       " 'S.C',\n",
       " 'Brazil\"',\n",
       " 'unbelievable',\n",
       " 'good',\n",
       " 'OK',\n",
       " 'lure',\n",
       " 'twig---a',\n",
       " 'ass,',\n",
       " 'Boone',\n",
       " 'situation',\n",
       " 'homoerotic',\n",
       " 'wastelands',\n",
       " 'writers/producers/director',\n",
       " 'cares',\n",
       " '(unintentionally?)',\n",
       " 'persuades',\n",
       " \"Burroughs'\",\n",
       " 'word,',\n",
       " 'Phoebe!!!',\n",
       " 'Gubra',\n",
       " 'jester.<br',\n",
       " 'admitted',\n",
       " 'larger-than-strife',\n",
       " 'airspeed',\n",
       " '8.5',\n",
       " 'hour',\n",
       " 'Magnificent',\n",
       " 'dimbulb',\n",
       " \"weren't\",\n",
       " 'cynics',\n",
       " 'Satan',\n",
       " 'convincing',\n",
       " 'streets',\n",
       " 'took',\n",
       " 'Chick',\n",
       " 'III:',\n",
       " \"story's\",\n",
       " 'showed',\n",
       " 'father',\n",
       " 'dialogs',\n",
       " 'coincides',\n",
       " 'Avenue,',\n",
       " 'allows',\n",
       " '(maybe',\n",
       " 'odd',\n",
       " 'lasted',\n",
       " 'Wind\"',\n",
       " 'incessant',\n",
       " 'credits,',\n",
       " '\"Seinfeld\".',\n",
       " 'whole',\n",
       " 'Blair',\n",
       " '(Brazil):',\n",
       " 'Grade:',\n",
       " 'one;',\n",
       " 'bit',\n",
       " 'enriched',\n",
       " 'Arquette',\n",
       " 'humbly',\n",
       " 'sub-human',\n",
       " 'interviewees',\n",
       " '(Janeane',\n",
       " 'inconsistency',\n",
       " 'There,',\n",
       " \"coming.'\",\n",
       " 'changes',\n",
       " 'numb.',\n",
       " 'forgetting',\n",
       " 'delicate',\n",
       " 'liners',\n",
       " 'moist,',\n",
       " 'nothingness.',\n",
       " 'first.',\n",
       " 'bugs).',\n",
       " 'prize,',\n",
       " 'obtaining',\n",
       " '(cheap)',\n",
       " 'insight',\n",
       " 'brother,',\n",
       " \"Fuhrer's\",\n",
       " 'warns',\n",
       " 'key',\n",
       " 'WITH',\n",
       " 'warning',\n",
       " 'admiring',\n",
       " 'ALL',\n",
       " 'LOTR',\n",
       " 'Todos',\n",
       " 'credit',\n",
       " 'this.<br',\n",
       " '/>After',\n",
       " 'ghetto',\n",
       " '(her',\n",
       " 'rafting',\n",
       " 'boat',\n",
       " 'place',\n",
       " \"Journey',\",\n",
       " 'plot-tool',\n",
       " 'melodrama',\n",
       " 'over-all',\n",
       " '(well',\n",
       " 'schmaltzy',\n",
       " '(Theresa',\n",
       " 'heroines',\n",
       " 'explanation.',\n",
       " 'inclinations',\n",
       " 'another--\"You',\n",
       " 'angel',\n",
       " 'devoted',\n",
       " 'non-linear',\n",
       " 'Orville',\n",
       " \"Kingsley's\",\n",
       " 'writers/producers',\n",
       " '(The',\n",
       " '(700',\n",
       " 'tension',\n",
       " 'basis',\n",
       " 'Stiffen',\n",
       " 'leads',\n",
       " 'ratcheting',\n",
       " 'exacerbated',\n",
       " '/>You',\n",
       " 'finalists',\n",
       " 'Waits',\n",
       " 'effect.<br',\n",
       " 'hand-to-hand',\n",
       " 'write!',\n",
       " 'abundance',\n",
       " 'dark',\n",
       " \"isn't.\",\n",
       " 'headings',\n",
       " 'afloat,',\n",
       " 'was!',\n",
       " 'communities,',\n",
       " 'amounts',\n",
       " '\"real',\n",
       " 'dwarf',\n",
       " 'noticed',\n",
       " 'language',\n",
       " 'trustees',\n",
       " '(yes,',\n",
       " 'me:',\n",
       " 'interesting.<br',\n",
       " 'threw',\n",
       " 'cultural',\n",
       " 'started',\n",
       " 'dialogue-driven.<br',\n",
       " 'Grayson.<br',\n",
       " 'mix--leaving',\n",
       " 'notwithstanding,',\n",
       " \"Jane's\",\n",
       " 'succeeded',\n",
       " 'bosom.Almoust',\n",
       " 'lesson',\n",
       " 'Buscemi,',\n",
       " 'tape',\n",
       " 'post',\n",
       " 'spots,',\n",
       " 'Africa',\n",
       " 'us,',\n",
       " 'digress...',\n",
       " 'Biscuits!\"',\n",
       " 'again.',\n",
       " 'Court',\n",
       " 'great!',\n",
       " 'JOHNSON!',\n",
       " 'effect',\n",
       " 'sloppy',\n",
       " \"'cute'\",\n",
       " 'remember,',\n",
       " 'adheres',\n",
       " 'either.Just',\n",
       " 'numerous',\n",
       " 'creative',\n",
       " 'Sant,',\n",
       " 'Anybody',\n",
       " 'travesty.',\n",
       " '/>So',\n",
       " 'wailing',\n",
       " 'there',\n",
       " 'hours,',\n",
       " 'single',\n",
       " 'shocking',\n",
       " 'affronts',\n",
       " 'scarily',\n",
       " 'Yasmin.',\n",
       " 'Louie.',\n",
       " 'How',\n",
       " 'confronted',\n",
       " 'Paulo',\n",
       " 'gratified.',\n",
       " '\"Danger',\n",
       " '/>Cary',\n",
       " 'swoons',\n",
       " 'Wood,',\n",
       " 'disease',\n",
       " 'lights',\n",
       " 'mistake.',\n",
       " '1976',\n",
       " 'answer,',\n",
       " 'solves',\n",
       " 'Tokyo',\n",
       " 'any?',\n",
       " 'intrigue,',\n",
       " 'an',\n",
       " 'It',\n",
       " 'defense',\n",
       " 'movie--on',\n",
       " 'Internet',\n",
       " 'Stefan',\n",
       " 'superhero,',\n",
       " 'Cinderella',\n",
       " 'turn.',\n",
       " 'unacceptable',\n",
       " 'losing',\n",
       " 'lower',\n",
       " 'cheeks\",',\n",
       " 'sister,',\n",
       " 'better',\n",
       " 'figure',\n",
       " 'advertising',\n",
       " 'dead.',\n",
       " 'applaud',\n",
       " 'interpret',\n",
       " 'minutes.<br',\n",
       " 'humor--just',\n",
       " 'Jewish',\n",
       " 'ambitious,',\n",
       " 'imaginative',\n",
       " \"Amanda's\",\n",
       " 'Stainton,',\n",
       " \"1950's.\",\n",
       " \"Claudia's\",\n",
       " 'television....though',\n",
       " 'principals',\n",
       " 'yes.',\n",
       " 'entire',\n",
       " 'tunnel',\n",
       " 'date.',\n",
       " 'anonymous',\n",
       " 'new',\n",
       " '\"So',\n",
       " 'landscapes',\n",
       " '/>Other',\n",
       " 'techniques,',\n",
       " 'into',\n",
       " 'off;',\n",
       " 'Borg',\n",
       " '\"Métro',\n",
       " 'geeky',\n",
       " 'MONEY.',\n",
       " '(Lisa',\n",
       " 'nature,',\n",
       " 'scripts,',\n",
       " 'together',\n",
       " '\"lost\"',\n",
       " 'Nash',\n",
       " 'paints',\n",
       " 'heartwarming',\n",
       " 'husband.',\n",
       " 'Karen.....',\n",
       " 'fact,Mr.',\n",
       " 'Not',\n",
       " '(soccer',\n",
       " 'Jeez,',\n",
       " 'Smacks',\n",
       " 'appreciative',\n",
       " 'Wonder',\n",
       " 'Ample',\n",
       " 'Michel',\n",
       " 'dry',\n",
       " 'works.The',\n",
       " 'glutton',\n",
       " '\"Midnight',\n",
       " 'Nico',\n",
       " 'Alex',\n",
       " 'realized',\n",
       " 'branch',\n",
       " 'matters',\n",
       " 'coverage',\n",
       " 'Hopkins',\n",
       " 'possibility',\n",
       " 'latest,',\n",
       " 'episode',\n",
       " 'contrived,',\n",
       " '\"Once',\n",
       " 'Truman',\n",
       " 'Talbott,',\n",
       " 'suspenseful',\n",
       " 'For',\n",
       " 'flourishes',\n",
       " 'noises.',\n",
       " '70',\n",
       " 'conveying',\n",
       " 'pulls',\n",
       " 'delivering',\n",
       " 'hitch-hikers(Juliette',\n",
       " 'school.',\n",
       " 'psycho-analysis,',\n",
       " 'water',\n",
       " 'looked',\n",
       " 'nurture',\n",
       " 'plausibility',\n",
       " 'locale',\n",
       " 'basing',\n",
       " 'luxuries',\n",
       " 'climbs',\n",
       " 'college',\n",
       " 'Witherspoon)',\n",
       " 'behaved',\n",
       " 'talent',\n",
       " 'too-clever',\n",
       " 'Sheedy',\n",
       " 'sugarcoated',\n",
       " 'both?',\n",
       " 'eyes',\n",
       " 'skimpy',\n",
       " 'Commercial',\n",
       " 'Morty,',\n",
       " 'awkwardly',\n",
       " 'Greene',\n",
       " 'movie-goers,it',\n",
       " 'Wilson)',\n",
       " \"hadn't\",\n",
       " 'peaking',\n",
       " 'staggers',\n",
       " 'technique',\n",
       " 'notoriety.',\n",
       " 'cinema?',\n",
       " 'beauty',\n",
       " 'arrived',\n",
       " 'generation',\n",
       " 'observation',\n",
       " 'Transsylvanian',\n",
       " 'Science-fiction',\n",
       " 'Henny',\n",
       " 'several',\n",
       " 'prison,',\n",
       " 'annoying',\n",
       " 'border',\n",
       " 'previously',\n",
       " 'robbed',\n",
       " 'time,',\n",
       " 'guitar',\n",
       " 'face?),',\n",
       " 'choppy,',\n",
       " 'Gay',\n",
       " 'humor',\n",
       " 'collections',\n",
       " 'Mamie',\n",
       " 'tears.',\n",
       " 'philosophical',\n",
       " 'gives',\n",
       " 'Richardson',\n",
       " 'Einstein',\n",
       " 'title',\n",
       " 'details',\n",
       " 'clips.',\n",
       " 'trend',\n",
       " 'TO',\n",
       " 'Time',\n",
       " 'Carpenter,',\n",
       " 'oracular,',\n",
       " 'outlaw',\n",
       " 'away',\n",
       " 'modernized',\n",
       " 'low-budget',\n",
       " 'icon',\n",
       " 'express',\n",
       " 'face',\n",
       " 'pictures',\n",
       " 'all.<br',\n",
       " 'lovers',\n",
       " 'tone-deaf',\n",
       " 'shortsightedness',\n",
       " 'usual.<br',\n",
       " 'tormented',\n",
       " 'problems,',\n",
       " 'seat',\n",
       " 'tramp',\n",
       " 'craving',\n",
       " 'grief,',\n",
       " 'layman',\n",
       " 'booze',\n",
       " '(Michael',\n",
       " 'mummies.<br',\n",
       " 'cutter',\n",
       " 'live',\n",
       " 'ass.',\n",
       " 'unhackneyed',\n",
       " 'panders',\n",
       " 'Shakespeare.',\n",
       " 'promised!<br',\n",
       " 'wooden',\n",
       " 'over-acting',\n",
       " 'Duvall',\n",
       " 'budget.<br',\n",
       " 'rant',\n",
       " 'UFOs,',\n",
       " 'semblance',\n",
       " 'connection.',\n",
       " 'vulgur,',\n",
       " 'follow',\n",
       " 'effective,',\n",
       " 'waste.',\n",
       " 'lawyer)',\n",
       " 'grand-outdoor-adventure',\n",
       " 'photographer',\n",
       " 'print',\n",
       " 'eccentric',\n",
       " 'improving.',\n",
       " 'werewolf-films',\n",
       " 'made-for-T.V.',\n",
       " 'Masala',\n",
       " 'it?',\n",
       " 'Jimmy',\n",
       " 'pure',\n",
       " 'tolerance',\n",
       " 'threaten',\n",
       " 'Superhero',\n",
       " 'F.',\n",
       " 'exploitive',\n",
       " 'Doug',\n",
       " 'Arthur',\n",
       " '1½',\n",
       " 'Eric',\n",
       " 'Leagues',\n",
       " 'fault',\n",
       " 'just',\n",
       " 'argue',\n",
       " 'free,',\n",
       " 'undertaking,',\n",
       " 'as,',\n",
       " 'self-defeating',\n",
       " 'presence',\n",
       " 'intimate',\n",
       " 'week,',\n",
       " 'KNACK',\n",
       " 'Simpson',\n",
       " 'speech',\n",
       " 'Soma.',\n",
       " 'Richard',\n",
       " 'De',\n",
       " 'Excruciatingly',\n",
       " 'lot',\n",
       " 'opponents.',\n",
       " 'him.<br',\n",
       " 'plot/dialogue',\n",
       " 'ARE',\n",
       " '\"Scream\"',\n",
       " 'act.',\n",
       " '(Lane',\n",
       " 'loud.',\n",
       " 'play.<br',\n",
       " 'set',\n",
       " 'feature,',\n",
       " 'belief,',\n",
       " \"ain't.<br\",\n",
       " 'House)',\n",
       " 'slacker',\n",
       " 'leave',\n",
       " 'Different',\n",
       " 'wonderful.',\n",
       " 'One',\n",
       " 'dorky',\n",
       " '`The',\n",
       " 'ciefly',\n",
       " 'casts',\n",
       " '(not',\n",
       " 'runway',\n",
       " 'cake',\n",
       " 'airplanes,',\n",
       " 'recommendations',\n",
       " 'Ritt',\n",
       " 'partner',\n",
       " 'mean,',\n",
       " 'commentary',\n",
       " 'failed',\n",
       " 'hostile',\n",
       " 'skill',\n",
       " 'film!',\n",
       " 'incredible.',\n",
       " 'excessive',\n",
       " 'bored',\n",
       " 'least,',\n",
       " 'Confederate',\n",
       " 'Beauty',\n",
       " 'excellence',\n",
       " 'impotent,',\n",
       " 'highest',\n",
       " 'qian',\n",
       " 'plight',\n",
       " 'consists',\n",
       " 'gorgeous',\n",
       " 'cook',\n",
       " 'rain,',\n",
       " 'crapulastic',\n",
       " 'Maureen',\n",
       " 'total',\n",
       " 'moment,',\n",
       " 'Rodgers',\n",
       " 'originals',\n",
       " 'bother',\n",
       " 'blackmail',\n",
       " 'entertains',\n",
       " 'headlines',\n",
       " 'endless',\n",
       " 'notice',\n",
       " 'titles',\n",
       " 'other...',\n",
       " 'Office.',\n",
       " 'to',\n",
       " 'violent',\n",
       " 'spends',\n",
       " 'Campbell,',\n",
       " 'aftermath',\n",
       " 'immediate',\n",
       " 'saves',\n",
       " 'silence',\n",
       " 'review',\n",
       " 'Oh...',\n",
       " 'feast.',\n",
       " 'fun,',\n",
       " 'focus.In',\n",
       " 'broken',\n",
       " 'Soles?)as',\n",
       " 'hungry',\n",
       " \"town's\",\n",
       " '/>Captain',\n",
       " 'husband.<br',\n",
       " 'self-centered,',\n",
       " \"you'd\",\n",
       " 'Kingdom',\n",
       " 'King.',\n",
       " 'ringing.',\n",
       " 'wider',\n",
       " 'kind',\n",
       " 'board?',\n",
       " '(Sam)',\n",
       " 'indication',\n",
       " '(\"Look!\"',\n",
       " 'testimony',\n",
       " 'Confused',\n",
       " 'course',\n",
       " 'clue',\n",
       " 'Kruger.',\n",
       " 'terrific.',\n",
       " '\"Well.',\n",
       " 'forever',\n",
       " '--',\n",
       " \"'reason'.\",\n",
       " \"O'Sullivan\",\n",
       " 'style.',\n",
       " 'picture,',\n",
       " 'pretensions',\n",
       " 'O-O;)',\n",
       " 'crone.',\n",
       " 'seriously.<br',\n",
       " 'original,',\n",
       " 'Wagner',\n",
       " 'Festival.<br',\n",
       " 'accordingly.',\n",
       " 'cipher',\n",
       " 'depression',\n",
       " 'homosexual',\n",
       " 'non-sensical',\n",
       " 'Shatta',\n",
       " 'werewolf',\n",
       " 'willing',\n",
       " 'throwing',\n",
       " 'aimed',\n",
       " 'sense),',\n",
       " 'fiercest',\n",
       " 'Escape',\n",
       " 'set-design',\n",
       " 'ride',\n",
       " 'cynical',\n",
       " 'trees',\n",
       " 'Gummi',\n",
       " 'connected',\n",
       " 's',\n",
       " 'outstanding',\n",
       " 'tiny',\n",
       " 'favored,',\n",
       " \"Monica's\",\n",
       " 'Aside',\n",
       " 'week!',\n",
       " 'Hmmmm',\n",
       " 'nomination',\n",
       " 'stands',\n",
       " 'lesbian',\n",
       " 'finale',\n",
       " 'searchlight',\n",
       " 'draaaaaaaawl',\n",
       " 'chanting',\n",
       " 'levels',\n",
       " '\"precious\"',\n",
       " 'debut,',\n",
       " 'Me,',\n",
       " 'engine',\n",
       " 'slightly',\n",
       " 'eats',\n",
       " 'trouble',\n",
       " '/>Steve',\n",
       " 'producers',\n",
       " 'faith',\n",
       " 'Story\"',\n",
       " 'shuttle',\n",
       " 'tribulations',\n",
       " 'Nicholas',\n",
       " 'filmmaker.<br',\n",
       " 'Virus',\n",
       " 'have',\n",
       " 'compiled',\n",
       " 'badly',\n",
       " 'soooo',\n",
       " 'effective.',\n",
       " '1989',\n",
       " 'incorporated',\n",
       " 'skin',\n",
       " 'sentences.',\n",
       " 'rascal.',\n",
       " 'complained',\n",
       " 'Victor',\n",
       " '10%',\n",
       " 'appropriate,',\n",
       " 'starting',\n",
       " 'disavows',\n",
       " 'cocktail',\n",
       " 'clearance',\n",
       " 'consisted',\n",
       " 'redemption',\n",
       " 'Rogers',\n",
       " 'edited,',\n",
       " 'Day-parade',\n",
       " 'ask',\n",
       " '\"Aliens\",',\n",
       " 'talks',\n",
       " 'sorry',\n",
       " 'typical',\n",
       " 'unfair,',\n",
       " 'watched,',\n",
       " 'sparring',\n",
       " 'venture.',\n",
       " 'children,',\n",
       " 'touches',\n",
       " 'Shakespearean',\n",
       " 'ever',\n",
       " 'needed',\n",
       " 'double',\n",
       " 'Melody',\n",
       " 'worn',\n",
       " 'cave',\n",
       " 'Because',\n",
       " 'it,\"',\n",
       " 'condemn',\n",
       " 'variation',\n",
       " 'exception.',\n",
       " 'networks',\n",
       " 'love.',\n",
       " 'movie).',\n",
       " 'quit',\n",
       " '\"X-Men\"',\n",
       " 'Meanwhile',\n",
       " '(to',\n",
       " '(3-Iron).',\n",
       " 'cue',\n",
       " 'demonstrate',\n",
       " '\"aliens\"',\n",
       " 'this.\"',\n",
       " 'human',\n",
       " 'purring',\n",
       " 'replace',\n",
       " 'widely',\n",
       " '\"film\"',\n",
       " 'deal!',\n",
       " '(such',\n",
       " 'Monopoly',\n",
       " 'Ironically',\n",
       " 'Gibbons,',\n",
       " 'upheld',\n",
       " 'rolling',\n",
       " 'suggestion:',\n",
       " 'on.<br',\n",
       " \"today's\",\n",
       " 'Sea\".',\n",
       " 'long.',\n",
       " 'ahead',\n",
       " 'GO',\n",
       " '(i.e.',\n",
       " '(this',\n",
       " 'old',\n",
       " 'correspond',\n",
       " 'rapist',\n",
       " 'start,',\n",
       " 'hookers',\n",
       " 'Kamm,',\n",
       " 'novel.',\n",
       " '(1989)',\n",
       " \"pilot's\",\n",
       " 'forty',\n",
       " 'week´s',\n",
       " 'agreements',\n",
       " \"I'll\",\n",
       " 'music,',\n",
       " '(can',\n",
       " 'awards,',\n",
       " 'can',\n",
       " 'them.',\n",
       " '\"A',\n",
       " 'filmmaking,',\n",
       " 'filled',\n",
       " 'perfection',\n",
       " 'mind),',\n",
       " 'admire',\n",
       " \"we're\",\n",
       " 'immortal',\n",
       " 'fluff.',\n",
       " 'baby',\n",
       " '(since',\n",
       " 'watch.<br',\n",
       " 'Park,',\n",
       " 'fifteen',\n",
       " 'tame,',\n",
       " 'standards,',\n",
       " 'Arquette),',\n",
       " 'edge',\n",
       " 'Garfield',\n",
       " '\"non-fiction',\n",
       " 'DEFINES',\n",
       " 'Everyone',\n",
       " 'work',\n",
       " 'counter',\n",
       " 'pray',\n",
       " 'Washington.',\n",
       " 'acting',\n",
       " 'portrayed,',\n",
       " 'characters.',\n",
       " 'Dafoe,',\n",
       " 'Vincent',\n",
       " 'well-made',\n",
       " 'Kline',\n",
       " 'fest',\n",
       " 'Kurosawa',\n",
       " 'upon',\n",
       " 'water.',\n",
       " 'drives',\n",
       " 'method',\n",
       " 'makeup,',\n",
       " 'village',\n",
       " 'Chicago,',\n",
       " 'Dumbrille',\n",
       " 'seller)',\n",
       " 'psycho',\n",
       " '1976.<br',\n",
       " 'technological',\n",
       " 'unusual.',\n",
       " 'newbies',\n",
       " 'spoofs',\n",
       " 'long',\n",
       " 'English,',\n",
       " 'hell',\n",
       " 'producing',\n",
       " 'alien',\n",
       " 'institution,',\n",
       " 'Corruption',\n",
       " '\\x96',\n",
       " 'charismatic',\n",
       " 'face!)',\n",
       " 'two.<br',\n",
       " 'woods,',\n",
       " 'Movies',\n",
       " '28,',\n",
       " 'habituated.',\n",
       " 'model,',\n",
       " 'hunter',\n",
       " 'Magnetic',\n",
       " 'them!',\n",
       " 'horrible',\n",
       " 'A',\n",
       " 'Pat',\n",
       " 'intentioned,',\n",
       " 'circuit',\n",
       " 'law',\n",
       " '/>My',\n",
       " 'ranks',\n",
       " 'hype',\n",
       " 'lack',\n",
       " 'detection',\n",
       " 'dares',\n",
       " 'Ryan,',\n",
       " 'romance.',\n",
       " 'Primus),',\n",
       " 'spice',\n",
       " '10/10,',\n",
       " 'brain,',\n",
       " 'age',\n",
       " 'endure',\n",
       " 'rating,',\n",
       " 'spread',\n",
       " 'Klangs).',\n",
       " '\"It\\'s',\n",
       " 'toy',\n",
       " 'paralleled',\n",
       " 'continues,',\n",
       " 'mildly.',\n",
       " 'garnered',\n",
       " 'value',\n",
       " 'billing,',\n",
       " 'excitement',\n",
       " 'neither.<br',\n",
       " 'SEASON',\n",
       " 'recently,',\n",
       " 'merit',\n",
       " 'passion',\n",
       " 'here,',\n",
       " 'fun.',\n",
       " 'increase',\n",
       " 'urgent',\n",
       " 'inception.',\n",
       " 'presented',\n",
       " 'Johnny',\n",
       " 'produces',\n",
       " 'history',\n",
       " 'acquire',\n",
       " 'guitars',\n",
       " 'Mary\"',\n",
       " 'cast,',\n",
       " 'aime',\n",
       " 'Joel',\n",
       " 'information.',\n",
       " 'desperately',\n",
       " 'window',\n",
       " '9/10.',\n",
       " 'contemptuously',\n",
       " 'Ethel',\n",
       " 'remained',\n",
       " 'wearing',\n",
       " 'Thou?)',\n",
       " 'ignores',\n",
       " 'Ellen',\n",
       " 'finds',\n",
       " 'millions',\n",
       " 'September',\n",
       " 'plain',\n",
       " 'Pollan',\n",
       " 'Glory!',\n",
       " 'brother-in-law',\n",
       " 'jet',\n",
       " 'ease',\n",
       " 'unafraid',\n",
       " 'explanation',\n",
       " 'killed,',\n",
       " '\"They\\'re',\n",
       " 'Each',\n",
       " 'Morty-legends,',\n",
       " 'Franchot',\n",
       " 'ravine',\n",
       " 'Berlin',\n",
       " 'hopefully',\n",
       " 'Manawaka,',\n",
       " 'dialogue,',\n",
       " 'above',\n",
       " 'During',\n",
       " 'event',\n",
       " 'self-indulgent',\n",
       " 'one...almost',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary():\n",
    "    vocabulary = set()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "build_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text through a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vocabulary, we can vectorize our documents. The value of our features will be the most simple vectorization of a document there is - the word counts.\n",
    "\n",
    "By doing this, each column value is the number of times that word of the vocabulary appeared in the document. This is what is called a **Bag of Words** (BoW) representation.\n",
    "\n",
    "Note that this type of vectorization of the document loses all the syntactic information of it. That is, you could shuffle the words in document and get the same vector (that's why it's called a bag of words). Of course, since we are trying to understand if a movie review is positive or negative, one could argue that what really matters as features is what kind of words appear in the documents and not their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize():\n",
    "    vocabulary = build_vocabulary()\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split(' ')\n",
    "        vector = np.array([doc.count(word) for word in vocabulary])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this better if we use a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1940.</th>\n",
       "      <th>world,</th>\n",
       "      <th>hubris</th>\n",
       "      <th>producer</th>\n",
       "      <th>mother's</th>\n",
       "      <th>refreshing,</th>\n",
       "      <th>me....NOTHING.&lt;br</th>\n",
       "      <th>Castleville.</th>\n",
       "      <th>describes</th>\n",
       "      <th>low-budget,</th>\n",
       "      <th>...</th>\n",
       "      <th>Caine</th>\n",
       "      <th>nothing.)</th>\n",
       "      <th>Crosscoe</th>\n",
       "      <th>anybody</th>\n",
       "      <th>Snake</th>\n",
       "      <th>film-making</th>\n",
       "      <th>close</th>\n",
       "      <th>that!)</th>\n",
       "      <th>boozy</th>\n",
       "      <th>see.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1940.  world,  hubris  producer  mother's  refreshing,  me....NOTHING.<br  \\\n",
       "0      0       0       0         0         0            0                  0   \n",
       "1      0       0       0         0         0            0                  0   \n",
       "2      0       0       0         0         0            0                  0   \n",
       "3      0       0       0         0         0            0                  0   \n",
       "4      0       0       0         0         0            0                  0   \n",
       "\n",
       "   Castleville.  describes  low-budget,  ...   Caine  nothing.)  Crosscoe  \\\n",
       "0             0          0            0  ...       0          0         0   \n",
       "1             0          0            0  ...       0          0         0   \n",
       "2             0          0            0  ...       0          0         0   \n",
       "3             0          0            0  ...       0          0         0   \n",
       "4             0          1            0  ...       0          0         0   \n",
       "\n",
       "   anybody  Snake  film-making  close  that!)  boozy  see.  \n",
       "0        0      0            0      0       0      0     0  \n",
       "1        0      0            0      0       0      0     0  \n",
       "2        0      0            0      0       0      0     0  \n",
       "3        0      0            0      0       0      0     0  \n",
       "4        1      0            0      0       0      0     0  \n",
       "\n",
       "[5 rows x 11503 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_df():\n",
    "    return pd.DataFrame(vectorize(), columns=build_vocabulary())\n",
    "\n",
    "build_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean this data a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hubris</th>\n",
       "      <th>producer</th>\n",
       "      <th>furry</th>\n",
       "      <th>setdesign</th>\n",
       "      <th>yo</th>\n",
       "      <th>describes</th>\n",
       "      <th>training</th>\n",
       "      <th>poland</th>\n",
       "      <th>unmissablebr</th>\n",
       "      <th>paid</th>\n",
       "      <th>...</th>\n",
       "      <th>elsebr</th>\n",
       "      <th>allude</th>\n",
       "      <th>groups</th>\n",
       "      <th>nominations</th>\n",
       "      <th>sebastien</th>\n",
       "      <th>anybody</th>\n",
       "      <th>mccarthy</th>\n",
       "      <th>close</th>\n",
       "      <th>boozy</th>\n",
       "      <th>pullitzer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7979 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hubris  producer  furry  setdesign  yo  describes  training  poland  \\\n",
       "0       0         0      0          0   3          0         0       0   \n",
       "1       0         0      0          0   0          0         0       0   \n",
       "2       0         0      0          0   2          0         0       0   \n",
       "3       0         0      0          0   3          0         0       0   \n",
       "4       0         0      0          0   6          1         0       0   \n",
       "\n",
       "   unmissablebr  paid    ...      elsebr  allude  groups  nominations  \\\n",
       "0             0     0    ...           0       0       0            0   \n",
       "1             0     0    ...           0       0       0            0   \n",
       "2             0     0    ...           0       0       0            0   \n",
       "3             0     0    ...           0       0       0            0   \n",
       "4             0     0    ...           0       0       0            0   \n",
       "\n",
       "   sebastien  anybody  mccarthy  close  boozy  pullitzer  \n",
       "0          0        0         0      0      0          0  \n",
       "1          0        0         0      0      0          0  \n",
       "2          0        0         0      0      0          0  \n",
       "3          0        0         0      0      0          0  \n",
       "4          1        1         0      0      0          0  \n",
       "\n",
       "[5 rows x 7979 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "docs = [re.sub(r'[\\d+'+string.punctuation+']', '', doc.lower()) for doc in docs]\n",
    "\n",
    "build_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking for the most meaningful features in our vocabulary to tell us in what category our document falls into. Text is filled with unimportant words to the meaning of a particular sentence like \"the\" or \"and\". This is in contrast with words like \"love\" or \"hate\" that have a very clear semantic meaning. The former example of words are called **stopwords** - words that don't introduce any meaning to a piece of text and are often just in document for syntactic reasons.\n",
    "\n",
    "You can easily find lists of stopwords for several languages in the internet. Here is a list for english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list came from here: http://snowball.tartarus.org/algorithms/english/stop.txt\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our build_vocabulary() and vectorize() functions and remove these words from the text. This way we will reduce our vocabulary - and thus our feature space - making our representations more lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hubris</th>\n",
       "      <th>producer</th>\n",
       "      <th>furry</th>\n",
       "      <th>setdesign</th>\n",
       "      <th>yo</th>\n",
       "      <th>describes</th>\n",
       "      <th>training</th>\n",
       "      <th>poland</th>\n",
       "      <th>unmissablebr</th>\n",
       "      <th>paid</th>\n",
       "      <th>...</th>\n",
       "      <th>elsebr</th>\n",
       "      <th>allude</th>\n",
       "      <th>groups</th>\n",
       "      <th>nominations</th>\n",
       "      <th>sebastien</th>\n",
       "      <th>anybody</th>\n",
       "      <th>mccarthy</th>\n",
       "      <th>close</th>\n",
       "      <th>boozy</th>\n",
       "      <th>lizitis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hubris  producer  furry  setdesign  yo  describes  training  poland  \\\n",
       "0       0         0      0          0   3          0         0       0   \n",
       "1       0         0      0          0   0          0         0       0   \n",
       "2       0         0      0          0   2          0         0       0   \n",
       "3       0         0      0          0   3          0         0       0   \n",
       "4       0         0      0          0   6          1         0       0   \n",
       "\n",
       "   unmissablebr  paid   ...     elsebr  allude  groups  nominations  \\\n",
       "0             0     0   ...          0       0       0            0   \n",
       "1             0     0   ...          0       0       0            0   \n",
       "2             0     0   ...          0       0       0            0   \n",
       "3             0     0   ...          0       0       0            0   \n",
       "4             0     0   ...          0       0       0            0   \n",
       "\n",
       "   sebastien  anybody  mccarthy  close  boozy  lizitis  \n",
       "0          0        0         0      0      0        0  \n",
       "1          0        0         0      0      0        0  \n",
       "2          0        0         0      0      0        0  \n",
       "3          0        0         0      0      0        0  \n",
       "4          1        1         0      0      0        0  \n",
       "\n",
       "[5 rows x 7850 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary():\n",
    "    vocabulary = set()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split() if word not in stopwords]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "def vectorize():\n",
    "    vocabulary = build_vocabulary()\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split(' ')\n",
    "        vector = np.array([doc.count(word) for word in vocabulary if word not in stopwords])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "BoW = build_df()\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we could do is to normalize our counts. As you can see, different documents have different number of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    771\n",
       "1     91\n",
       "2    471\n",
       "3    457\n",
       "4    886\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can introduce bias in our features, so we should normalize each document by its number of words. This way, instead of having word counts as features of our model, we will have **term frequencies**. This way, the features in any document of the dataset sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = BoW.div(BoW.sum(axis=1), axis=0)\n",
    "tf.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kicking it up a notch with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear by now that not all words have the same importance to find out in what category a document falls into.\n",
    "\n",
    "In our dataset, if we want to classify a review as positive, for instance, the word \"_good_\" is much more informative than \"_house_\", and we should give it more weight as a feature.\n",
    "\n",
    "In general, words that are very common in the corpus are less informative than rare words.\n",
    "\n",
    "That is the rational behind **Term Frequency - Inverse Document Frequency (TF-IDF)**:\n",
    "$$ tfidf _{t, d} =(1+log_2{(tf_{t,d})})*(1+log_2{(\\frac{N}{df_{t}})})  $$\n",
    "\n",
    "where $t$ and $d$ are the term and document for which we are computing a feature, $tf_{t,d}$ is the term frequency of term $t$ in document $d$, $N$ is the total number of documents we have, while $df_{t}$ is the number of documents that contain $t$.\n",
    "\n",
    "We are using the word frequencies we were using before, but now we are weighting each by the inverse of the number of times they occur in all the documents. The more a word a appears in a document and the less it appears in other documents, the higher is the TF-IDF of that word in that document.\n",
    "\n",
    "In short, we measure *the term frequency, weighted by its rarity in the entire corpus*.\n",
    "\n",
    "In practice, the $tf_{t,d}$ part of the equation is going to lead to a lot of zeros, since not all terms are in all documents. Since if a term exists in a document the equation will always wield a number different from 0, we only compute the $tfidf _{t, d}$ if $t$ exists in document $d$. Otherwise, we will define that $tfidf=0$. This is usually done with a sparse implementation of vectors, which we will have later, where we end up only computing values for the terms that appear in a document. Since we don't have that at the moment, we will just replace the -inf we get from taking the $log_2{(0)}$ with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goncalocorreia/miniconda2/envs/dsa/lib/python3.5/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log2\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hubris</th>\n",
       "      <th>producer</th>\n",
       "      <th>furry</th>\n",
       "      <th>setdesign</th>\n",
       "      <th>yo</th>\n",
       "      <th>describes</th>\n",
       "      <th>training</th>\n",
       "      <th>poland</th>\n",
       "      <th>unmissablebr</th>\n",
       "      <th>paid</th>\n",
       "      <th>...</th>\n",
       "      <th>elsebr</th>\n",
       "      <th>allude</th>\n",
       "      <th>groups</th>\n",
       "      <th>nominations</th>\n",
       "      <th>sebastien</th>\n",
       "      <th>anybody</th>\n",
       "      <th>mccarthy</th>\n",
       "      <th>close</th>\n",
       "      <th>boozy</th>\n",
       "      <th>lizitis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.829463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.634626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.663082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-9.593694</td>\n",
       "      <td>-67.198385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-75.989548</td>\n",
       "      <td>-55.5771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hubris  producer  furry  setdesign         yo  describes  training  poland  \\\n",
       "0     0.0       0.0    0.0        0.0 -10.829463   0.000000       0.0     0.0   \n",
       "1     0.0       0.0    0.0        0.0   0.000000   0.000000       0.0     0.0   \n",
       "2     0.0       0.0    0.0        0.0 -10.634626   0.000000       0.0     0.0   \n",
       "3     0.0       0.0    0.0        0.0  -9.663082   0.000000       0.0     0.0   \n",
       "4     0.0       0.0    0.0        0.0  -9.593694 -67.198385       0.0     0.0   \n",
       "\n",
       "   unmissablebr  paid   ...     elsebr  allude  groups  nominations  \\\n",
       "0           0.0   0.0   ...        0.0     0.0     0.0          0.0   \n",
       "1           0.0   0.0   ...        0.0     0.0     0.0          0.0   \n",
       "2           0.0   0.0   ...        0.0     0.0     0.0          0.0   \n",
       "3           0.0   0.0   ...        0.0     0.0     0.0          0.0   \n",
       "4           0.0   0.0   ...        0.0     0.0     0.0          0.0   \n",
       "\n",
       "   sebastien  anybody  mccarthy  close  boozy  lizitis  \n",
       "0   0.000000   0.0000       0.0    0.0    0.0      0.0  \n",
       "1   0.000000   0.0000       0.0    0.0    0.0      0.0  \n",
       "2   0.000000   0.0000       0.0    0.0    0.0      0.0  \n",
       "3   0.000000   0.0000       0.0    0.0    0.0      0.0  \n",
       "4 -75.989548 -55.5771       0.0    0.0    0.0      0.0  \n",
       "\n",
       "[5 rows x 7850 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import inf\n",
    "\n",
    "def idf(column):\n",
    "    return 1 + np.log2(len(column) / sum(column > 0))\n",
    "\n",
    "tf_idf = (1 + np.log2(tf)).multiply(tf.apply(idf))\n",
    "tf_idf[tf_idf==-inf]=0\n",
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing we can do with our feature representations is to check similarities between words. To do this, instead of seeing the vocabulary as the features of a given document, we see each document as a feature of a given term in the vocabulary. By doing this, we get a vectorized representation of a word!\n",
    "\n",
    "A very popular way of computing similarities between vectors is to compute the cosine similarity.\n",
    "\n",
    "Let's check the similarity between the word _good_ and the word _great_ in our Bag of Words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31382296]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(BoW['good'].__array__().reshape(1,-1), BoW['great'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute it for _good_ and _awful_. We should get a lower similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20225996]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(BoW['good'].__array__().reshape(1,-1), BoW['awful'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the same similarity scores but in the tf-idf representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42873222]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tf_idf['good'].__array__().reshape(1,-1), tf_idf['great'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18866055]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tf_idf['good'].__array__().reshape(1,-1), tf_idf['awful'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The gap between the similarities of these pair of words increased with our tf-idf representation. This means that our tf-idf model is computing better and more meaningful features than our BoW model. This will surely help when we feed these feature matrices to a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using all of this in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, there are a lot of implementations of what we learned above in the internet. We're going to use scikit learn from now to compute these feature representations.\n",
    "\n",
    "Our BoW representations, for instance, can be done with scikit's CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small sample of the vocabulary: ['skyler', 'dementia', 'inciteful', 'teletype', 'laundrette', 'landru', 'comity', 'israle', 'scope', 'sten', 'tomasso', 'jerseys', 'halo', 'investments', 'degrade', 'unhappily', 'taut', 'zavattini', 'kumar', 'marverick']\n",
      "\n",
      "Number of distinct words: 74849\n"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(df['text'].values)\n",
    "\n",
    "# Looking at a small sample of the vocabulary:\n",
    "vocabulary = list(vectorizer.vocabulary_.keys())\n",
    "print(\"Small sample of the vocabulary:\", vocabulary[0:20])\n",
    "\n",
    "# Number of words in the vocabulary\n",
    "print(\"\\nNumber of distinct words:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kudos to Cesar Montano for reviving the Cebuano movie! Panaghoy sa Suba is very good -- it has the drama, the action, the romance, and scene that will make you laugh.<br /><br />While the story is not that original (a love triangle -- or make a four-cornered-love, Japanese occupation, rebellion, American as lord), its presentation is something cool, especially it uses it original language -- bisaya for the Filipino, nipongo for the Japanese and English for the American.<br /><br />This movie will go as one of this year's best Pinoy movies.<br /><br />Go watch this! \n",
      "\n",
      "action :  1\n",
      "american :  2\n",
      "and :  2\n",
      "as :  2\n",
      "best :  1\n",
      "bisaya :  1\n",
      "br :  6\n",
      "cebuano :  1\n",
      "cesar :  1\n",
      "cool :  1\n",
      "cornered :  1\n",
      "drama :  1\n",
      "english :  1\n",
      "especially :  1\n",
      "filipino :  1\n",
      "for :  4\n",
      "four :  1\n",
      "go :  2\n",
      "good :  1\n",
      "has :  1\n",
      "is :  3\n",
      "it :  3\n",
      "its :  1\n",
      "japanese :  2\n",
      "kudos :  1\n",
      "language :  1\n",
      "laugh :  1\n",
      "lord :  1\n",
      "love :  2\n",
      "make :  2\n",
      "montano :  1\n",
      "movie :  2\n",
      "movies :  1\n",
      "nipongo :  1\n",
      "not :  1\n",
      "occupation :  1\n",
      "of :  1\n",
      "one :  1\n",
      "or :  1\n",
      "original :  2\n",
      "panaghoy :  1\n",
      "pinoy :  1\n",
      "presentation :  1\n",
      "rebellion :  1\n",
      "reviving :  1\n",
      "romance :  1\n",
      "sa :  1\n",
      "scene :  1\n",
      "something :  1\n",
      "story :  1\n",
      "suba :  1\n",
      "that :  2\n",
      "the :  8\n",
      "this :  3\n",
      "to :  1\n",
      "triangle :  1\n",
      "uses :  1\n",
      "very :  1\n",
      "watch :  1\n",
      "while :  1\n",
      "will :  2\n",
      "year :  1\n",
      "you :  1\n"
     ]
    }
   ],
   "source": [
    "sentence = df['text'].values[12:13]\n",
    "print(sentence[0], '\\n')\n",
    "\n",
    "# Tranform sentence into bag of words representation\n",
    "word_count_sentence = vectorizer.transform(sentence)\n",
    "\n",
    "# Find the indexes of the words which appear in the sentence\n",
    "_, columns = word_count_sentence.nonzero()\n",
    "\n",
    "# Get the inverse map to map vector indexes to words\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "inv_map = {v: k for k, v in vocabulary.items()}\n",
    "\n",
    "# Extract the corresponding word and count\n",
    "counts = [(inv_map[i], word_count_sentence[0, i]) for i in columns]\n",
    "\n",
    "for word, count in counts:\n",
    "    print(word, \": \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 74849)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_matrix = vectorizer.transform(df['text'].values)\n",
    "word_count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And tf-idf can be done with TfidfTransformer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(word_count_matrix)\n",
    "\n",
    "word_term_frequency_matrix = tfidf.transform(word_count_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
